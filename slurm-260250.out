12/09/2019 14:13:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/09/2019 14:13:14 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-config.json from cache at /h/mtian/.cache/torch/transformers/4f6b2675253400aebc8ab1673ec27add127daf362af404ae81bfc57880c04d85.4e4ae0b75092dc73403e23e1652c47d6f6ab1993188fa8862fb72b943ba13157
12/09/2019 14:13:14 - INFO - transformers.configuration_utils -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "finetuning_task": null,
  "gap_size": 0,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_labels": 2,
  "num_memory_blocks": 0,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30000
}

12/09/2019 14:13:14 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-spiece.model from cache at /h/mtian/.cache/torch/transformers/6a228bf1ddec144cf2e94c6c762dfce1b721a082cf98437db1c62172bbd4d6c3.c81d4deb77aec08ce575b7a39a989a79dd54f321bfb82c2b54dd35f52f8182cf
12/09/2019 14:13:14 - WARNING - transformers.modeling_utils -   There is currently an upstream reproducibility issue with ALBERT v2 models. Please see https://github.com/google-research/google-research/issues/119 for more information.
12/09/2019 14:13:14 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-pytorch_model.bin from cache at /h/mtian/.cache/torch/transformers/e6fd1be612c6797d1dc556e26724c9ed6ea0a45b79e3486533eccf9432a23ce6.857dc1dec4c7a7f2a7a0088f4139c2424ecc3d9a238f5c02de82f24a1eb69439
12/09/2019 14:13:15 - INFO - transformers.modeling_utils -   Weights of AlbertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']
12/09/2019 14:13:15 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in AlbertForQuestionAnswering: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']
12/09/2019 14:13:19 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, beta1=0.9, beta2=0.98, cache_dir='', config_name='', device=device(type='cuda'), do_eval=False, do_lower_case=False, do_train=True, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=16, learning_rate=3e-05, local_rank=-1, logging_steps=50, lr_scheduler='cosine', max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=512, max_steps=1000, model_name_or_path='albert-large-v2', model_type='albert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='./output/models/albert_bioasq_max_beta2_98_from_scratch/', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=3, per_gpu_train_batch_size=3, predict_file='/scratch/gobi1/mtian/BioASQ/BioASQ-test-factoid-4b-1.json', save_steps=100, seed=42, server_ip='', server_port='', tokenizer_name='', train_file='/scratch/gobi1/mtian/BioASQ/BioASQ-train-factoid-4b.json', verbose_logging=False, version_2_with_negative=False, warmup_steps=100, weight_decay=0.0)
12/09/2019 14:13:19 - INFO - __main__ -   Loading features from cached file /scratch/gobi1/mtian/BioASQ/cached_train_albert-large-v2_512
12/09/2019 14:13:20 - INFO - __main__ -   ***** Running training *****
12/09/2019 14:13:20 - INFO - __main__ -     Num examples = 4542
12/09/2019 14:13:20 - INFO - __main__ -     Num Epochs = 11
12/09/2019 14:13:20 - INFO - __main__ -     Instantaneous batch size per GPU = 3
12/09/2019 14:13:20 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 48
12/09/2019 14:13:20 - INFO - __main__ -     Gradient Accumulation steps = 16
12/09/2019 14:13:20 - INFO - __main__ -     Total optimization steps = 1000
Epoch:   0%|          | 0/11 [00:00<?, ?it/s]
Iteration:   0%|          | 0/1514 [00:00<?, ?it/s][A
Iteration:   0%|          | 1/1514 [00:00<13:38,  1.85it/s][A
Iteration:   0%|          | 2/1514 [00:01<13:04,  1.93it/s][A
Iteration:   0%|          | 3/1514 [00:01<12:41,  1.98it/s][A
Iteration:   0%|          | 4/1514 [00:01<12:23,  2.03it/s][A
Iteration:   0%|          | 5/1514 [00:02<12:12,  2.06it/s][A
Iteration:   0%|          | 6/1514 [00:02<12:03,  2.09it/s][A
Iteration:   0%|          | 7/1514 [00:03<11:57,  2.10it/s][A
Iteration:   1%|          | 8/1514 [00:03<11:53,  2.11it/s][A
Iteration:   1%|          | 9/1514 [00:04<11:49,  2.12it/s][A
Iteration:   1%|          | 10/1514 [00:04<11:48,  2.12it/s][A
Iteration:   1%|          | 11/1514 [00:05<11:47,  2.12it/s][A
Iteration:   1%|          | 12/1514 [00:05<11:46,  2.13it/s][A
Iteration:   1%|          | 13/1514 [00:06<11:45,  2.13it/s][A
Iteration:   1%|          | 14/1514 [00:06<11:44,  2.13it/s][A
Iteration:   1%|          | 15/1514 [00:07<11:44,  2.13it/s][A
Iteration:   1%|          | 16/1514 [00:07<11:47,  2.12it/s][A
Iteration:   1%|          | 17/1514 [00:08<11:46,  2.12it/s][A
Iteration:   1%|          | 18/1514 [00:08<11:45,  2.12it/s][A
Iteration:   1%|▏         | 19/1514 [00:08<11:45,  2.12it/s][A
Iteration:   1%|▏         | 20/1514 [00:09<11:43,  2.12it/s][A
Iteration:   1%|▏         | 21/1514 [00:09<11:43,  2.12it/s][A
Iteration:   1%|▏         | 22/1514 [00:10<11:42,  2.12it/s][A
Iteration:   2%|▏         | 23/1514 [00:10<11:42,  2.12it/s][A
Iteration:   2%|▏         | 24/1514 [00:11<11:41,  2.12it/s][A
Iteration:   2%|▏         | 25/1514 [00:11<11:41,  2.12it/s][A
Iteration:   2%|▏         | 26/1514 [00:12<11:41,  2.12it/s][A
Iteration:   2%|▏         | 27/1514 [00:12<11:40,  2.12it/s][A
Iteration:   2%|▏         | 28/1514 [00:13<11:40,  2.12it/s][A
Iteration:   2%|▏         | 29/1514 [00:13<11:40,  2.12it/s][A
Iteration:   2%|▏         | 30/1514 [00:14<11:40,  2.12it/s][A
Iteration:   2%|▏         | 31/1514 [00:14<11:39,  2.12it/s][A
Iteration:   2%|▏         | 32/1514 [00:15<11:41,  2.11it/s][A
Iteration:   2%|▏         | 33/1514 [00:15<11:40,  2.11it/s][A
Iteration:   2%|▏         | 34/1514 [00:16<11:39,  2.12it/s][A
Iteration:   2%|▏         | 35/1514 [00:16<11:39,  2.12it/s][A
Iteration:   2%|▏         | 36/1514 [00:17<11:38,  2.11it/s][A
Iteration:   2%|▏         | 37/1514 [00:17<11:37,  2.12it/s][A
Iteration:   3%|▎         | 38/1514 [00:17<11:37,  2.12it/s][A
Iteration:   3%|▎         | 39/1514 [00:18<11:37,  2.12it/s][A
Iteration:   3%|▎         | 40/1514 [00:18<11:37,  2.11it/s][A
Iteration:   3%|▎         | 41/1514 [00:19<11:37,  2.11it/s][A
Iteration:   3%|▎         | 42/1514 [00:19<11:36,  2.11it/s][A
Iteration:   3%|▎         | 43/1514 [00:20<11:36,  2.11it/s][A
Iteration:   3%|▎         | 44/1514 [00:20<11:35,  2.11it/s][A
Iteration:   3%|▎         | 45/1514 [00:21<11:35,  2.11it/s][A
Iteration:   3%|▎         | 46/1514 [00:21<11:35,  2.11it/s][A
Iteration:   3%|▎         | 47/1514 [00:22<11:35,  2.11it/s][A
Iteration:   3%|▎         | 48/1514 [00:22<11:37,  2.10it/s][A
Iteration:   3%|▎         | 49/1514 [00:23<11:35,  2.11it/s][A
Iteration:   3%|▎         | 50/1514 [00:23<11:35,  2.10it/s][A
Iteration:   3%|▎         | 51/1514 [00:24<11:35,  2.10it/s][A
Iteration:   3%|▎         | 52/1514 [00:24<11:34,  2.10it/s][A
Iteration:   4%|▎         | 53/1514 [00:25<11:34,  2.10it/s][A
Iteration:   4%|▎         | 54/1514 [00:25<11:34,  2.10it/s][A
Iteration:   4%|▎         | 55/1514 [00:26<11:34,  2.10it/s][A
Iteration:   4%|▎         | 56/1514 [00:26<11:33,  2.10it/s][A
Iteration:   4%|▍         | 57/1514 [00:26<11:33,  2.10it/s][A
Iteration:   4%|▍         | 58/1514 [00:27<11:32,  2.10it/s][A
Iteration:   4%|▍         | 59/1514 [00:27<11:32,  2.10it/s][A
Iteration:   4%|▍         | 60/1514 [00:28<11:31,  2.10it/s][A
Iteration:   4%|▍         | 61/1514 [00:28<11:31,  2.10it/s][A
Iteration:   4%|▍         | 62/1514 [00:29<11:31,  2.10it/s][A
Iteration:   4%|▍         | 63/1514 [00:29<11:31,  2.10it/s][A
Iteration:   4%|▍         | 64/1514 [00:30<11:33,  2.09it/s][A
Iteration:   4%|▍         | 65/1514 [00:30<11:33,  2.09it/s][A
Iteration:   4%|▍         | 66/1514 [00:31<11:31,  2.09it/s][A
Iteration:   4%|▍         | 67/1514 [00:31<11:31,  2.09it/s][A
Iteration:   4%|▍         | 68/1514 [00:32<11:30,  2.10it/s][A
Iteration:   5%|▍         | 69/1514 [00:32<11:29,  2.10it/s][A
Iteration:   5%|▍         | 70/1514 [00:33<11:29,  2.10it/s][A
Iteration:   5%|▍         | 71/1514 [00:33<11:29,  2.09it/s][A
Iteration:   5%|▍         | 72/1514 [00:34<11:28,  2.09it/s][A
Iteration:   5%|▍         | 73/1514 [00:34<11:28,  2.09it/s][A
Iteration:   5%|▍         | 74/1514 [00:35<11:27,  2.09it/s][A
Iteration:   5%|▍         | 75/1514 [00:35<11:27,  2.09it/s][A
Iteration:   5%|▌         | 76/1514 [00:36<11:26,  2.09it/s][A
Iteration:   5%|▌         | 77/1514 [00:36<11:27,  2.09it/s][A
Iteration:   5%|▌         | 78/1514 [00:37<11:26,  2.09it/s][A
Iteration:   5%|▌         | 79/1514 [00:37<11:26,  2.09it/s][A
Iteration:   5%|▌         | 80/1514 [00:37<11:28,  2.08it/s][A
Iteration:   5%|▌         | 81/1514 [00:38<11:26,  2.09it/s][A
Iteration:   5%|▌         | 82/1514 [00:38<11:26,  2.09it/s][A
Iteration:   5%|▌         | 83/1514 [00:39<11:26,  2.08it/s][A
Iteration:   6%|▌         | 84/1514 [00:39<11:25,  2.09it/s][A
Iteration:   6%|▌         | 85/1514 [00:40<11:25,  2.08it/s][A
Iteration:   6%|▌         | 86/1514 [00:40<11:24,  2.09it/s][A
Iteration:   6%|▌         | 87/1514 [00:41<11:24,  2.08it/s][A
Iteration:   6%|▌         | 88/1514 [00:41<11:23,  2.09it/s][A
Iteration:   6%|▌         | 89/1514 [00:42<11:23,  2.08it/s][A
Iteration:   6%|▌         | 90/1514 [00:42<11:23,  2.08it/s][A
Iteration:   6%|▌         | 91/1514 [00:43<11:23,  2.08it/s][A
Iteration:   6%|▌         | 92/1514 [00:43<11:23,  2.08it/s][A
Iteration:   6%|▌         | 93/1514 [00:44<11:22,  2.08it/s][A
Iteration:   6%|▌         | 94/1514 [00:44<11:22,  2.08it/s][A
01/23/2020 15:28:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/23/2020 15:28:39 - INFO - transformers.configuration_utils -   loading configuration file ./output/models/bioasq_albert_v2_lre3-5/config.json
01/23/2020 15:28:39 - INFO - transformers.configuration_utils -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "finetuning_task": null,
  "gap_size": 0,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_labels": 2,
  "num_memory_blocks": 0,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30000
}

01/23/2020 15:28:39 - INFO - transformers.tokenization_utils -   Model name './output/models/bioasq_albert_v2_lre3-5/' not found in model shortcut name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). Assuming './output/models/bioasq_albert_v2_lre3-5/' is a path or url to a directory containing tokenizer files.
01/23/2020 15:28:39 - INFO - transformers.tokenization_utils -   loading file ./output/models/bioasq_albert_v2_lre3-5/spiece.model
01/23/2020 15:28:39 - INFO - transformers.tokenization_utils -   loading file ./output/models/bioasq_albert_v2_lre3-5/added_tokens.json
01/23/2020 15:28:39 - INFO - transformers.tokenization_utils -   loading file ./output/models/bioasq_albert_v2_lre3-5/special_tokens_map.json
01/23/2020 15:28:39 - INFO - transformers.tokenization_utils -   loading file ./output/models/bioasq_albert_v2_lre3-5/tokenizer_config.json
01/23/2020 15:28:39 - WARNING - transformers.modeling_utils -   There is currently an upstream reproducibility issue with ALBERT v2 models. Please see https://github.com/google-research/google-research/issues/119 for more information.
01/23/2020 15:28:39 - INFO - transformers.modeling_utils -   loading weights file ./output/models/bioasq_albert_v2_lre3-5/pytorch_model.bin
01/23/2020 15:28:44 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, beta1=0.9, beta2=0.999, cache_dir='', config_name='', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=6, learning_rate=3e-05, local_rank=-1, logging_steps=50, lr_scheduler='linear', max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='./output/models/bioasq_albert_v2_lre3-5/', model_type='albert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=5.0, output_dir='./output/models/albert_bioasq/4b1/', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=2, per_gpu_train_batch_size=2, predict_file='dir/BioASQ-test-factoid-4b-1.json', save_steps=30000, seed=42, server_ip='', server_port='', tokenizer_name='', train_file='dir/BioASQ-train-factoid-4b.json', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
01/23/2020 15:28:44 - INFO - __main__ -   Evaluate the following checkpoints: ['./output/models/albert_bioasq/4b1/']
Traceback (most recent call last):
  File "/h/mtian/.local/lib/python3.6/site-packages/transformers/configuration_utils.py", line 134, in from_pretrained
    resolved_config_file = cached_path(config_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies)
  File "/h/mtian/.local/lib/python3.6/site-packages/transformers/file_utils.py", line 182, in cached_path
    raise EnvironmentError("file {} not found".format(url_or_filename))
OSError: file ./output/models/albert_bioasq/4b1/ not found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./examples/run_squad_max.py", line 587, in <module>
    main()
  File "./examples/run_squad_max.py", line 572, in main
    model = model_class.from_pretrained(checkpoint)
  File "/h/mtian/.local/lib/python3.6/site-packages/transformers/modeling_utils.py", line 337, in from_pretrained
    **kwargs
  File "/h/mtian/.local/lib/python3.6/site-packages/transformers/configuration_utils.py", line 146, in from_pretrained
    raise EnvironmentError(msg)
OSError: Model name './output/models/albert_bioasq/4b1/' was not found in model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed './output/models/albert_bioasq/4b1/' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url.

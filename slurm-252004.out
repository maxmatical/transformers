usage: run_squad_max.py [-h] --train_file TRAIN_FILE --predict_file
                        PREDICT_FILE --model_type MODEL_TYPE
                        --model_name_or_path MODEL_NAME_OR_PATH --output_dir
                        OUTPUT_DIR [--config_name CONFIG_NAME]
                        [--tokenizer_name TOKENIZER_NAME]
                        [--cache_dir CACHE_DIR] [--version_2_with_negative]
                        [--null_score_diff_threshold NULL_SCORE_DIFF_THRESHOLD]
                        [--max_seq_length MAX_SEQ_LENGTH]
                        [--doc_stride DOC_STRIDE]
                        [--max_query_length MAX_QUERY_LENGTH] [--do_train]
                        [--do_eval] [--evaluate_during_training]
                        [--do_lower_case]
                        [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                        [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                        [--learning_rate LEARNING_RATE]
                        [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                        [--weight_decay WEIGHT_DECAY] [--beta1 BETA1]
                        [--beta2 BETA2] [--adam_epsilon ADAM_EPSILON]
                        [--max_grad_norm MAX_GRAD_NORM]
                        [--num_train_epochs NUM_TRAIN_EPOCHS]
                        [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]
                        [--n_best_size N_BEST_SIZE]
                        [--max_answer_length MAX_ANSWER_LENGTH]
                        [--verbose_logging] [--logging_steps LOGGING_STEPS]
                        [--save_steps SAVE_STEPS] [--eval_all_checkpoints]
                        [--no_cuda] [--overwrite_output_dir]
                        [--overwrite_cache] [--seed SEED]
                        [--local_rank LOCAL_RANK] [--fp16]
                        [--fp16_opt_level FP16_OPT_LEVEL]
                        [--server_ip SERVER_IP] [--server_port SERVER_PORT]
                        [--lr_scheduler LR_SCHEDULER]
run_squad_max.py: error: the following arguments are required: --train_file, --predict_file, --model_type, --model_name_or_path, --output_dir
11/30/2019 02:41:25 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
11/30/2019 02:41:25 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-config.json from cache at /h/mtian/.cache/torch/transformers/4f6b2675253400aebc8ab1673ec27add127daf362af404ae81bfc57880c04d85.4e4ae0b75092dc73403e23e1652c47d6f6ab1993188fa8862fb72b943ba13157
11/30/2019 02:41:25 - INFO - transformers.configuration_utils -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "finetuning_task": null,
  "gap_size": 0,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_labels": 2,
  "num_memory_blocks": 0,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30000
}

11/30/2019 02:41:25 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-spiece.model from cache at /h/mtian/.cache/torch/transformers/6a228bf1ddec144cf2e94c6c762dfce1b721a082cf98437db1c62172bbd4d6c3.c81d4deb77aec08ce575b7a39a989a79dd54f321bfb82c2b54dd35f52f8182cf
11/30/2019 02:41:26 - WARNING - transformers.modeling_utils -   There is currently an upstream reproducibility issue with ALBERT v2 models. Please see https://github.com/google-research/google-research/issues/119 for more information.
11/30/2019 02:41:26 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-pytorch_model.bin from cache at /h/mtian/.cache/torch/transformers/e6fd1be612c6797d1dc556e26724c9ed6ea0a45b79e3486533eccf9432a23ce6.857dc1dec4c7a7f2a7a0088f4139c2424ecc3d9a238f5c02de82f24a1eb69439
11/30/2019 02:41:27 - INFO - transformers.modeling_utils -   Weights of AlbertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']
11/30/2019 02:41:27 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in AlbertForQuestionAnswering: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']
11/30/2019 02:41:27 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, beta1=0.9, beta2=0.98, cache_dir='', config_name='', device=device(type='cpu'), do_eval=True, do_lower_case=False, do_train=True, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=16, learning_rate=3e-05, local_rank=-1, logging_steps=50, lr_scheduler='cosine', max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=512, max_steps=5000, model_name_or_path='albert-large-v2', model_type='albert', n_best_size=20, n_gpu=0, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=2.0, output_dir='./output/models/albert_squad_max_beta2_98/', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=3, per_gpu_train_batch_size=3, predict_file='/scratch/gobi1/mtian/SQUAD/dev-v2.0.json', save_steps=500, seed=42, server_ip='', server_port='', tokenizer_name='', train_file='/scratch/gobi1/mtian/SQUAD/train-v2.0.json', verbose_logging=False, version_2_with_negative=True, warmup_steps=500, weight_decay=0.0)
11/30/2019 02:41:27 - INFO - __main__ -   Loading features from cached file /scratch/gobi1/mtian/SQUAD/cached_train_albert-large-v2_512
11/30/2019 02:42:00 - INFO - __main__ -   ***** Running training *****
11/30/2019 02:42:00 - INFO - __main__ -     Num examples = 132998
11/30/2019 02:42:00 - INFO - __main__ -     Num Epochs = 2
11/30/2019 02:42:00 - INFO - __main__ -     Instantaneous batch size per GPU = 3
11/30/2019 02:42:00 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 48
11/30/2019 02:42:00 - INFO - __main__ -     Gradient Accumulation steps = 16
11/30/2019 02:42:00 - INFO - __main__ -     Total optimization steps = 5000
Epoch:   0%|          | 0/2 [00:00<?, ?it/s]
Iteration:   0%|          | 0/44333 [00:00<?, ?it/s][A
Iteration:   0%|          | 1/44333 [00:43<529:54:48, 43.03s/it][A
Iteration:   0%|          | 2/44333 [01:55<639:08:43, 51.90s/it][A
Iteration:   0%|          | 3/44333 [03:02<693:07:52, 56.29s/it][A
Iteration:   0%|          | 4/44333 [04:16<759:08:54, 61.65s/it][A
Iteration:   0%|          | 5/44333 [05:42<847:53:35, 68.86s/it][A
Iteration:   0%|          | 6/44333 [07:10<918:39:11, 74.61s/it][A
Iteration:   0%|          | 7/44333 [08:37<965:05:02, 78.38s/it][A
Iteration:   0%|          | 8/44333 [09:53<957:31:41, 77.77s/it][A
Iteration:   0%|          | 9/44333 [11:33<1038:49:55, 84.37s/it][A
Iteration:   0%|          | 10/44333 [13:05<1067:41:12, 86.72s/it][A
Iteration:   0%|          | 11/44333 [14:29<1057:25:54, 85.89s/it][A
Iteration:   0%|          | 12/44333 [15:51<1041:44:41, 84.62s/it][A
Iteration:   0%|          | 13/44333 [17:20<1059:48:51, 86.09s/it][A
Iteration:   0%|          | 14/44333 [18:39<1031:30:03, 83.79s/it][A
Iteration:   0%|          | 15/44333 [19:57<1010:11:57, 82.06s/it][A
Iteration:   0%|          | 16/44333 [21:10<978:43:22, 79.50s/it] [A
Iteration:   0%|          | 17/44333 [22:24<958:51:25, 77.89s/it][A
Iteration:   0%|          | 18/44333 [23:47<975:22:16, 79.24s/it][A
Iteration:   0%|          | 19/44333 [25:09<988:28:50, 80.30s/it][A
Iteration:   0%|          | 20/44333 [26:24<966:49:02, 78.54s/it][A
Iteration:   0%|          | 21/44333 [27:36<944:09:17, 76.71s/it][A
Iteration:   0%|          | 22/44333 [28:50<931:47:58, 75.70s/it][A
Iteration:   0%|          | 23/44333 [30:05<930:43:53, 75.62s/it][A
Iteration:   0%|          | 24/44333 [31:15<911:05:55, 74.02s/it][A
Iteration:   0%|          | 25/44333 [32:36<934:11:13, 75.90s/it][A
Iteration:   0%|          | 26/44333 [33:54<943:13:24, 76.64s/it][A
Iteration:   0%|          | 27/44333 [35:15<960:38:24, 78.05s/it][A
Iteration:   0%|          | 28/44333 [36:26<934:27:37, 75.93s/it][A
Iteration:   0%|          | 29/44333 [37:42<932:01:05, 75.73s/it][A
Iteration:   0%|          | 30/44333 [38:49<900:03:53, 73.14s/it][A
Iteration:   0%|          | 31/44333 [40:03<905:43:40, 73.60s/it][A
Iteration:   0%|          | 32/44333 [41:16<903:27:18, 73.42s/it][A
Iteration:   0%|          | 33/44333 [42:36<927:35:10, 75.38s/it][A
Iteration:   0%|          | 34/44333 [43:53<933:36:21, 75.87s/it][A
Iteration:   0%|          | 35/44333 [45:07<924:23:44, 75.12s/it][A
Iteration:   0%|          | 36/44333 [46:13<892:56:48, 72.57s/it][A
Iteration:   0%|          | 37/44333 [47:21<875:30:55, 71.15s/it][A
Iteration:   0%|          | 38/44333 [48:31<868:47:46, 70.61s/it][A
Iteration:   0%|          | 39/44333 [49:43<877:02:27, 71.28s/it][A
Iteration:   0%|          | 40/44333 [50:52<867:26:53, 70.50s/it][A
Iteration:   0%|          | 41/44333 [52:07<882:35:53, 71.74s/it][A
Iteration:   0%|          | 42/44333 [53:22<897:08:08, 72.92s/it][A
Iteration:   0%|          | 43/44333 [54:32<884:23:32, 71.89s/it][A
Iteration:   0%|          | 44/44333 [55:42<876:49:16, 71.27s/it][A
Iteration:   0%|          | 45/44333 [57:00<901:05:36, 73.25s/it][A
Iteration:   0%|          | 46/44333 [58:16<911:57:04, 74.13s/it][A
Iteration:   0%|          | 47/44333 [59:31<915:00:07, 74.38s/it][A
Iteration:   0%|          | 48/44333 [1:00:40<897:57:50, 73.00s/it][A
Iteration:   0%|          | 49/44333 [1:01:53<897:18:46, 72.95s/it][A
Iteration:   0%|          | 50/44333 [1:03:12<918:02:28, 74.63s/it][A
Iteration:   0%|          | 51/44333 [1:04:31<935:51:11, 76.08s/it][A
Iteration:   0%|          | 52/44333 [1:05:42<914:42:21, 74.36s/it][A
Iteration:   0%|          | 53/44333 [1:06:59<925:51:42, 75.27s/it][A
Iteration:   0%|          | 54/44333 [1:08:10<908:34:42, 73.87s/it][A
Iteration:   0%|          | 55/44333 [1:09:29<930:34:41, 75.66s/it][A
Iteration:   0%|          | 56/44333 [1:10:47<938:04:52, 76.27s/it][A
Iteration:   0%|          | 57/44333 [1:12:07<950:14:52, 77.26s/it][A
Iteration:   0%|          | 58/44333 [1:13:28<966:32:16, 78.59s/it][A
Iteration:   0%|          | 59/44333 [1:14:51<981:39:18, 79.82s/it][A
Iteration:   0%|          | 60/44333 [1:16:35<1070:52:55, 87.08s/it][A
Iteration:   0%|          | 61/44333 [1:17:42<995:29:18, 80.95s/it] [A
Iteration:   0%|          | 62/44333 [1:19:00<983:37:17, 79.99s/it][A
Iteration:   0%|          | 63/44333 [1:20:14<964:01:03, 78.39s/it][A
Iteration:   0%|          | 64/44333 [1:21:32<960:01:53, 78.07s/it][A
Iteration:   0%|          | 65/44333 [1:22:48<954:42:05, 77.64s/it][A
Iteration:   0%|          | 66/44333 [1:24:09<965:59:22, 78.56s/it][A
Iteration:   0%|          | 67/44333 [1:25:50<1047:24:57, 85.18s/it][A
Iteration:   0%|          | 68/44333 [1:27:17<1055:40:09, 85.86s/it][A
Iteration:   0%|          | 69/44333 [1:28:27<998:21:55, 81.20s/it] [A
Iteration:   0%|          | 70/44333 [1:29:44<982:43:07, 79.93s/it][A
Iteration:   0%|          | 71/44333 [1:30:59<962:44:39, 78.30s/it][A
Iteration:   0%|          | 72/44333 [1:32:17<960:53:55, 78.16s/it][A
Iteration:   0%|          | 73/44333 [1:33:28<935:32:24, 76.09s/it][A
Iteration:   0%|          | 74/44333 [1:34:41<924:20:18, 75.19s/it][A
Iteration:   0%|          | 75/44333 [1:35:57<927:56:15, 75.48s/it][A
Iteration:   0%|          | 76/44333 [1:37:14<935:06:12, 76.06s/it][A
Iteration:   0%|          | 77/44333 [1:38:33<945:53:43, 76.94s/it][A
Iteration:   0%|          | 78/44333 [1:39:53<953:40:43, 77.58s/it][A
Iteration:   0%|          | 79/44333 [1:41:08<944:08:49, 76.81s/it][A
Iteration:   0%|          | 80/44333 [1:42:22<936:38:41, 76.20s/it][A
Iteration:   0%|          | 81/44333 [1:43:36<927:48:14, 75.48s/it][A
Iteration:   0%|          | 82/44333 [1:44:51<924:41:06, 75.23s/it][A
Iteration:   0%|          | 83/44333 [1:46:00<901:42:56, 73.36s/it][A
Iteration:   0%|          | 84/44333 [1:47:12<895:52:43, 72.89s/it][A
Iteration:   0%|          | 85/44333 [1:48:20<879:28:54, 71.55s/it][A
Iteration:   0%|          | 86/44333 [1:49:37<901:13:04, 73.32s/it][A
Iteration:   0%|          | 87/44333 [1:50:55<916:30:37, 74.57s/it][A
Iteration:   0%|          | 88/44333 [1:52:20<957:00:55, 77.87s/it][A
Iteration:   0%|          | 89/44333 [1:53:41<966:50:45, 78.67s/it][A
Iteration:   0%|          | 90/44333 [1:54:52<939:43:11, 76.46s/it][A
Iteration:   0%|          | 91/44333 [1:56:03<919:09:41, 74.79s/it][A
Iteration:   0%|          | 92/44333 [1:57:19<923:29:08, 75.15s/it][A
Iteration:   0%|          | 93/44333 [1:58:30<908:32:26, 73.93s/it][A
Iteration:   0%|          | 94/44333 [1:59:45<912:02:29, 74.22s/it][A
Iteration:   0%|          | 95/44333 [2:00:55<894:58:42, 72.83s/it][A
Iteration:   0%|          | 96/44333 [2:02:09<901:45:23, 73.38s/it][A
Iteration:   0%|          | 97/44333 [2:03:23<902:46:52, 73.47s/it][A
Iteration:   0%|          | 98/44333 [2:04:36<900:41:37, 73.30s/it][A
Iteration:   0%|          | 99/44333 [2:05:50<903:56:39, 73.57s/it][A
Iteration:   0%|          | 100/44333 [2:07:05<907:05:43, 73.83s/it][A
Iteration:   0%|          | 101/44333 [2:08:15<893:52:00, 72.75s/it][A
Iteration:   0%|          | 102/44333 [2:09:29<898:25:01, 73.12s/it][A
Iteration:   0%|          | 103/44333 [2:10:44<906:35:54, 73.79s/it][A
Iteration:   0%|          | 104/44333 [2:11:55<894:24:02, 72.80s/it][A
Iteration:   0%|          | 105/44333 [2:13:07<890:42:29, 72.50s/it][A
Iteration:   0%|          | 106/44333 [2:14:17<882:29:32, 71.83s/it][A
Iteration:   0%|          | 107/44333 [2:15:21<855:09:14, 69.61s/it][A
Iteration:   0%|          | 108/44333 [2:16:32<858:21:35, 69.87s/it][A
Iteration:   0%|          | 109/44333 [2:17:42<859:20:38, 69.95s/it][A
Iteration:   0%|          | 110/44333 [2:18:47<842:36:41, 68.59s/it][A
Iteration:   0%|          | 111/44333 [2:19:58<851:51:30, 69.35s/it][A
Iteration:   0%|          | 112/44333 [2:21:07<847:34:20, 69.00s/it][A
Iteration:   0%|          | 113/44333 [2:22:28<893:16:10, 72.72s/it][A
Iteration:   0%|          | 114/44333 [2:23:46<913:49:52, 74.40s/it][A
Iteration:   0%|          | 115/44333 [2:24:55<892:23:15, 72.65s/it][A
Iteration:   0%|          | 116/44333 [2:26:06<888:06:37, 72.31s/it][A
Iteration:   0%|          | 117/44333 [2:27:25<909:41:49, 74.07s/it][A
Iteration:   0%|          | 118/44333 [2:28:34<893:06:17, 72.72s/it][A
Iteration:   0%|          | 119/44333 [2:29:44<881:37:18, 71.78s/it][A
Iteration:   0%|          | 120/44333 [2:30:58<891:53:24, 72.62s/it][A
Iteration:   0%|          | 121/44333 [2:32:08<881:22:36, 71.77s/it][A
Iteration:   0%|          | 122/44333 [2:33:24<896:07:03, 72.97s/it][A
Iteration:   0%|          | 123/44333 [2:34:43<917:56:04, 74.75s/it][A
Iteration:   0%|          | 124/44333 [2:35:57<917:07:50, 74.68s/it][A
Iteration:   0%|          | 125/44333 [2:37:10<911:19:18, 74.21s/it][A
Iteration:   0%|          | 126/44333 [2:38:45<985:08:01, 80.22s/it][A
Iteration:   0%|          | 127/44333 [2:40:01<969:30:18, 78.95s/it][A
Iteration:   0%|          | 128/44333 [2:41:14<950:08:18, 77.38s/it][A
Iteration:   0%|          | 129/44333 [2:42:37<969:25:15, 78.95s/it][A
Iteration:   0%|          | 130/44333 [2:43:50<946:27:01, 77.08s/it][A
Iteration:   0%|          | 131/44333 [2:45:02<927:24:05, 75.53s/it][A
Iteration:   0%|          | 132/44333 [2:46:12<907:10:44, 73.89s/it][A
Iteration:   0%|          | 133/44333 [2:47:11<853:23:18, 69.51s/it][A
Iteration:   0%|          | 134/44333 [2:48:12<823:15:02, 67.05s/it][A
Iteration:   0%|          | 135/44333 [2:49:22<831:47:21, 67.75s/it][A
Iteration:   0%|          | 136/44333 [2:50:20<796:30:53, 64.88s/it][A
Iteration:   0%|          | 137/44333 [2:51:25<798:20:34, 65.03s/it][A
Iteration:   0%|          | 138/44333 [2:52:31<801:46:26, 65.31s/it][A
Iteration:   0%|          | 139/44333 [2:53:50<850:06:06, 69.25s/it][A
Iteration:   0%|          | 140/44333 [2:54:50<818:40:13, 66.69s/it][A
Iteration:   0%|          | 141/44333 [2:55:57<817:48:58, 66.62s/it][A
Iteration:   0%|          | 142/44333 [2:57:01<809:39:29, 65.96s/it][A
Iteration:   0%|          | 143/44333 [2:58:12<827:35:10, 67.42s/it][A
Iteration:   0%|          | 144/44333 [2:59:23<840:24:19, 68.47s/it][A
Iteration:   0%|          | 145/44333 [3:00:22<805:18:29, 65.61s/it][A
Iteration:   0%|          | 146/44333 [3:01:33<824:06:56, 67.14s/it][A
Iteration:   0%|          | 147/44333 [3:02:49<856:38:43, 69.79s/it][A
Iteration:   0%|          | 148/44333 [3:03:52<833:05:54, 67.88s/it][A
Iteration:   0%|          | 149/44333 [3:04:57<821:33:57, 66.94s/it][A
Iteration:   0%|          | 150/44333 [3:06:00<809:48:32, 65.98s/it][A
Iteration:   0%|          | 151/44333 [3:07:09<817:20:26, 66.60s/it][A
Iteration:   0%|          | 152/44333 [3:08:21<838:44:40, 68.34s/it][A
Iteration:   0%|          | 153/44333 [3:09:21<809:31:42, 65.96s/it][A
Iteration:   0%|          | 154/44333 [3:10:19<778:13:22, 63.41s/it][A
Iteration:   0%|          | 155/44333 [3:11:22<778:43:30, 63.46s/it][A
Iteration:   0%|          | 156/44333 [3:12:27<781:24:38, 63.68s/it][A
Iteration:   0%|          | 157/44333 [3:13:33<791:54:50, 64.53s/it][A
Iteration:   0%|          | 158/44333 [3:14:39<797:58:27, 65.03s/it][A
Iteration:   0%|          | 159/44333 [3:15:48<809:48:46, 66.00s/it][A
Iteration:   0%|          | 160/44333 [3:16:52<803:14:56, 65.46s/it][A
Iteration:   0%|          | 161/44333 [3:17:54<792:05:29, 64.56s/it][A
Iteration:   0%|          | 162/44333 [3:19:00<795:32:01, 64.84s/it][A
Iteration:   0%|          | 163/44333 [3:20:08<808:28:39, 65.89s/it][A
Iteration:   0%|          | 164/44333 [3:21:10<793:59:56, 64.71s/it][A
Iteration:   0%|          | 165/44333 [3:22:16<799:30:43, 65.17s/it][A
Iteration:   0%|          | 166/44333 [3:23:18<786:08:57, 64.08s/it][A
Iteration:   0%|          | 167/44333 [3:24:17<770:08:21, 62.77s/it][A
Iteration:   0%|          | 168/44333 [3:25:19<765:19:25, 62.38s/it][A
Iteration:   0%|          | 169/44333 [3:26:22<767:33:21, 62.57s/it][A
Iteration:   0%|          | 170/44333 [3:27:28<778:44:04, 63.48s/it][A
Iteration:   0%|          | 171/44333 [3:28:29<772:53:41, 63.00s/it][A
Iteration:   0%|          | 172/44333 [3:29:29<761:55:22, 62.11s/it][A
Iteration:   0%|          | 173/44333 [3:30:31<760:37:06, 62.01s/it][A
Iteration:   0%|          | 174/44333 [3:31:32<754:56:22, 61.55s/it][A
Iteration:   0%|          | 175/44333 [3:32:35<761:54:43, 62.12s/it][A
Iteration:   0%|          | 176/44333 [3:33:38<765:58:08, 62.45s/it][A
Iteration:   0%|          | 177/44333 [3:34:40<763:47:47, 62.27s/it][A
Iteration:   0%|          | 178/44333 [3:35:49<785:56:27, 64.08s/it][A
Iteration:   0%|          | 179/44333 [3:36:52<785:02:13, 64.01s/it][A
Iteration:   0%|          | 180/44333 [3:37:54<775:55:29, 63.26s/it][A
Iteration:   0%|          | 181/44333 [3:38:58<779:43:26, 63.58s/it][A
Iteration:   0%|          | 182/44333 [3:40:01<778:22:39, 63.47s/it][A
Iteration:   0%|          | 183/44333 [3:41:00<760:34:25, 62.02s/it][A
Iteration:   0%|          | 184/44333 [3:41:59<749:39:06, 61.13s/it][A
Iteration:   0%|          | 185/44333 [3:43:01<752:37:29, 61.37s/it][A
Iteration:   0%|          | 186/44333 [3:44:04<759:26:29, 61.93s/it][A
Iteration:   0%|          | 187/44333 [3:45:03<746:49:05, 60.90s/it][A
Iteration:   0%|          | 188/44333 [3:46:02<739:19:57, 60.29s/it][A
Iteration:   0%|          | 189/44333 [3:47:09<765:03:27, 62.39s/it][A
Iteration:   0%|          | 190/44333 [3:48:12<765:52:24, 62.46s/it][A
Iteration:   0%|          | 191/44333 [3:49:11<755:33:20, 61.62s/it][A
Iteration:   0%|          | 192/44333 [3:50:09<739:44:16, 60.33s/it][A
Iteration:   0%|          | 193/44333 [3:51:11<746:56:15, 60.92s/it][A
Iteration:   0%|          | 194/44333 [3:52:19<774:48:18, 63.19s/it][A
Iteration:   0%|          | 195/44333 [3:53:23<776:05:05, 63.30s/it][A
Iteration:   0%|          | 196/44333 [3:54:28<781:13:38, 63.72s/it][A
Iteration:   0%|          | 197/44333 [3:55:37<802:16:09, 65.44s/it][A
Iteration:   0%|          | 198/44333 [3:56:49<827:12:57, 67.47s/it][A
Iteration:   0%|          | 199/44333 [3:57:53<815:06:22, 66.49s/it][A
Iteration:   0%|          | 200/44333 [3:58:54<793:11:51, 64.70s/it][A
Iteration:   0%|          | 201/44333 [3:59:58<789:39:36, 64.42s/it][A
Iteration:   0%|          | 202/44333 [4:00:56<766:27:19, 62.52s/it][A
Iteration:   0%|          | 203/44333 [4:01:57<760:23:09, 62.03s/it][A
Iteration:   0%|          | 204/44333 [4:03:06<788:38:10, 64.34s/it][A
Iteration:   0%|          | 205/44333 [4:04:07<776:21:10, 63.34s/it][A
Iteration:   0%|          | 206/44333 [4:05:07<762:00:16, 62.17s/it][A
Iteration:   0%|          | 207/44333 [4:06:18<794:18:54, 64.80s/it][A
Iteration:   0%|          | 208/44333 [4:08:57<1139:42:22, 92.98s/it][A
Iteration:   0%|          | 209/44333 [4:12:00<1470:53:54, 120.01s/it][A
Iteration:   0%|          | 210/44333 [4:13:06<1275:00:22, 104.03s/it][A
Iteration:   0%|          | 211/44333 [4:14:16<1148:12:49, 93.69s/it] [A
Iteration:   0%|          | 212/44333 [4:15:23<1048:38:12, 85.56s/it][A
Iteration:   0%|          | 213/44333 [4:16:28<974:55:00, 79.55s/it] [A
Iteration:   0%|          | 214/44333 [4:17:43<957:03:23, 78.09s/it][A
Iteration:   0%|          | 215/44333 [4:18:55<934:50:12, 76.28s/it][A
Iteration:   0%|          | 216/44333 [4:20:03<904:41:43, 73.82s/it][A
Iteration:   0%|          | 217/44333 [4:21:07<870:44:19, 71.05s/it][A
Iteration:   0%|          | 218/44333 [4:22:15<858:35:42, 70.07s/it][A
Iteration:   0%|          | 219/44333 [4:23:20<837:36:22, 68.35s/it][A
Iteration:   0%|          | 220/44333 [4:24:41<886:59:05, 72.39s/it][A
Iteration:   0%|          | 221/44333 [4:25:41<841:30:21, 68.68s/it][A
Iteration:   1%|          | 222/44333 [4:26:47<828:55:12, 67.65s/it][A
Iteration:   1%|          | 223/44333 [4:28:47<1021:08:47, 83.34s/it][A
Iteration:   1%|          | 224/44333 [4:30:26<1079:51:22, 88.13s/it][A
Iteration:   1%|          | 225/44333 [4:32:12<1145:13:52, 93.47s/it][A/var/spool/slurmd/job252004/slurm_script: line 33: 47426 Killed                  python3 ./examples/run_squad_max.py --model_type albert --model_name_or_path albert-large-v2 --do_train --do_eval --version_2_with_negative --train_file /scratch/gobi1/mtian/SQUAD/train-v2.0.json --predict_file /scratch/gobi1/mtian/SQUAD/dev-v2.0.json --learning_rate 3e-5 --weight_decay 0 --beta1 0.9 --beta2 0.98 --adam_epsilon 1e-8 --lr_scheduler 'cosine' --num_train_epochs 2 --max_steps 5000 --save_steps 500 --warmup_steps 500 --max_seq_length 512 --doc_stride 128 --output_dir ./output/models/albert_squad_max_beta2_98/ --overwrite_output_dir --gradient_accumulation_steps 16 --per_gpu_eval_batch_size=3 --per_gpu_train_batch_size=3
slurmstepd: error: Detected 1 oom-kill event(s) in step 252004.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.

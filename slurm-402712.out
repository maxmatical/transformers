02/07/2020 07:31:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
02/07/2020 07:31:08 - INFO - transformers.configuration_utils -   loading configuration file /scratch/gobi1/mtian/models/bert_squad_max_beta2_98_lr_3e-5/config.json
02/07/2020 07:31:08 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

02/07/2020 07:31:08 - INFO - transformers.tokenization_utils -   Model name '/scratch/gobi1/mtian/models/bert_squad_max_beta2_98_lr_3e-5/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/scratch/gobi1/mtian/models/bert_squad_max_beta2_98_lr_3e-5/' is a path or url to a directory containing tokenizer files.
02/07/2020 07:31:08 - INFO - transformers.tokenization_utils -   loading file /scratch/gobi1/mtian/models/bert_squad_max_beta2_98_lr_3e-5/vocab.txt
02/07/2020 07:31:08 - INFO - transformers.tokenization_utils -   loading file /scratch/gobi1/mtian/models/bert_squad_max_beta2_98_lr_3e-5/added_tokens.json
02/07/2020 07:31:08 - INFO - transformers.tokenization_utils -   loading file /scratch/gobi1/mtian/models/bert_squad_max_beta2_98_lr_3e-5/special_tokens_map.json
02/07/2020 07:31:08 - INFO - transformers.tokenization_utils -   loading file /scratch/gobi1/mtian/models/bert_squad_max_beta2_98_lr_3e-5/tokenizer_config.json
02/07/2020 07:31:08 - INFO - transformers.modeling_utils -   loading weights file /scratch/gobi1/mtian/models/bert_squad_max_beta2_98_lr_3e-5/pytorch_model.bin
02/07/2020 07:31:20 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, beta1=0.9, beta2=0.98, cache_dir='', config_name='', device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=16, learning_rate=5e-06, local_rank=-1, logging_steps=50, lr_scheduler='cosine', max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/scratch/gobi1/mtian/models/bert_squad_max_beta2_98_lr_3e-5/', model_type='bert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='/scratch/gobi1/mtian/models/bioasq_bert_1epoch_lre5-6_sq/', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=2, per_gpu_train_batch_size=2, predict_file='/scratch/gobi1/mtian/BioASQ/BioASQ-test-factoid-4b-1.json', save_steps=50, seed=42, server_ip='', server_port='', tokenizer_name='', train_file='/scratch/gobi1/mtian/BioASQ/BioASQ-train-factoid-4b.json', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
02/07/2020 07:31:20 - INFO - __main__ -   Loading features from cached file /scratch/gobi1/mtian/BioASQ/cached_train_bert_squad_max_beta2_98_lr_3e-5_384
slurmstepd: error: *** JOB 402712 ON guppy7 CANCELLED AT 2020-02-07T07:31:20 ***
